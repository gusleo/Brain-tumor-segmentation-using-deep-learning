{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "vqtJYVB2MfB3",
    "outputId": "c1c5154f-5e6e-4baa-caf5-edfe1efc6977"
   },
   "outputs": [],
   "source": [
    "#!pip install SimpleITK \n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-image\n",
    "#!pip install keras==2.2.5\n",
    "#!pip install tqdm\n",
    "#!pip install pandas opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "8dSfUsh_MWUo",
    "outputId": "9fc3cbf7-3ba1-48d1-a3f5-80bec079ae5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import skimage.color as color\n",
    "import random as r\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers import Input, merge, UpSampling2D,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from tensorflow.contrib.tpu.python.tpu import keras_support\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.models import load_model\n",
    "import pandas\n",
    "import cv2\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FK3N1QIlMWUv",
    "outputId": "f32e0edb-5c76-4894-a400-9f67fb86b00b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0: other\\n1: necrosis + NET\\n2: edema\\n4: enhancing tumor\\n5: full tumor\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_image_data_format(\"channels_first\")\n",
    "experiment = \"LRelu\"\n",
    "dataset_path = '../MICCAI_BraTS17_Data_Training/';\n",
    "destination_numpy = './Dataset' \n",
    "destination_model = './model' \n",
    "result_destination = './Result/{}'.format(experiment)\n",
    "\n",
    "batch_size = 10\n",
    "start_epoch = 15\n",
    "continue_training = False\n",
    "img_size = 240      #original img size is 240*240\n",
    "smooth = 0.005 \n",
    "num_of_aug = 2\n",
    "num_epoch = start_epoch + 35\n",
    "pul_seq = 'Flair'\n",
    "sharp = False       # sharpen filter\n",
    "LR = 1e-4\n",
    "#verbose = 0 if (num_epoch - start_epoch) > 30 else 1\n",
    "verbose = 1\n",
    "num_of_patch = 4 #must be a square number\n",
    "label_num = 5   # 1 = necrosis+NET, 2 = tumor core, 3= original, 4 = ET, 5 = complete tumor\n",
    "'''\n",
    "0: other\n",
    "1: necrosis + NET\n",
    "2: edema\n",
    "4: enhancing tumor\n",
    "5: full tumor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pM5qEcTdA6i"
   },
   "outputs": [],
   "source": [
    "def extract_zip(path_zip, destination):\n",
    "  import zipfile  # For faster extraction\n",
    "  dataset_path = path_zip  # Replace with your dataset path\n",
    "  zfile = zipfile.ZipFile(dataset_path)\n",
    "  zfile.extractall(destination)\n",
    "  print(\"Extract Zip file done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Etbu0J2OdFJG"
   },
   "outputs": [],
   "source": [
    "#if os.path.isdir('/content/gdrive/My Drive/Thesis/MICCAI_BraTS17_Data_Training/') == False:\n",
    "  #extract_zip(\"/content/gdrive/My Drive/Thesis/MICCAI_BraTS17_Data_Training_IPP.zip\", \"/content/gdrive/My Drive/Thesis/\")\n",
    "#if os.path.isdir('/content/gdrive/My Drive/Thesis/N4DataValid/') == False:\n",
    "  #extract_zip(\"/content/gdrive/My Drive/Thesis/N4DataValid.zip\", \"/content/gdrive/My Drive/Thesis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjfFAPPbMWU2"
   },
   "outputs": [],
   "source": [
    "# function to read all data (training and label) and transform into numpy array\n",
    "\n",
    "def create_data(src, mask, scanType=\"flair\", augmented=False, label_num = 0, label=False, resize=(155,img_size,img_size)):\n",
    "    files = glob.glob(src + mask, recursive=True)\n",
    "    files.sort(reverse=True)\n",
    "    r.seed(9)\n",
    "    r.shuffle(files)    # shuffle patients\n",
    "    imgs = []\n",
    "    print('Processing---', mask)\n",
    "    for file in tqdm(files):\n",
    "        img = io.imread(file, plugin='simpleitk')\n",
    "        #img = trans.resize(img, resize, mode='constant')\n",
    "        if label:\n",
    "            if label_num == 5:\n",
    "                img[img != 0] = 1       #Region 1 => 1+2+3+4 complete tumor\n",
    "            if label_num == 1:\n",
    "                img[img != 1] = 0       #only left necrosis and NET\n",
    "            if label_num == 2:\n",
    "                img[img == 2] = 0       #turn edema to 0\n",
    "                img[img != 0] = 1       #only keep necrosis, ET, NET = Tumor core\n",
    "            if label_num == 4:\n",
    "                img[img != 4] = 0       #only left ET\n",
    "                img[img == 4] = 1\n",
    "            if label_num == 3:\n",
    "                img[img == 3] = 1       # remain GT, design for 2015 data\n",
    "                \n",
    "                \n",
    "            img = img.astype('float32')\n",
    "        else:\n",
    "            img = (img-img.mean()) / img.std()      #normalization => zero mean   !!!care for the std=0 problem\n",
    "            img = img.astype('float32')\n",
    "        for slice in range(60,130):     #choose the slice range\n",
    "            img_t = img[slice,:,:]\n",
    "            img_t =img_t.reshape((1,)+img_t.shape)\n",
    "            img_t =img_t.reshape((1,)+img_t.shape)   #become rank 4\n",
    "            if augmented == True:\n",
    "              img_g = augmentation(img_t,num_of_aug)\n",
    "            for n in range(img_t.shape[0]):\n",
    "                if augmented == True:\n",
    "                  imgs.append(img_g[n,:,:,:])\n",
    "                else:\n",
    "                  imgs.append(img_t[n,:,:,:])\n",
    "    name = 'y_'+ str(img_size) if label else 'x_'+ str(img_size)\n",
    "    if label == False:\n",
    "      name = name + '_' + scanType\n",
    "    else:\n",
    "      name = name + '_' + str(label_num)\n",
    "    np.save('{}/{}.npy'.format(destination_numpy, name), np.array(imgs).astype('float32'))  # save at home\n",
    "    print('Saved', len(files), 'to', name)\n",
    "    del imgs\n",
    "    gc.collect()\n",
    "    #return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcLeeH36MWU6"
   },
   "outputs": [],
   "source": [
    "#function to read one subject data\n",
    "def create_data_onesubject_val(src, mask, count, label_num = 0, label=False):\n",
    "    files = glob.glob(src + mask, recursive=True)\n",
    "    #r.seed(9)\n",
    "    #r.shuffle(files)    # shuffle patients\n",
    "    k = len(files) - count -1\n",
    "    imgs = []\n",
    "    file = files[k]\n",
    "    print('Processing---', mask,'--',file)\n",
    "    \n",
    "    img = io.imread(file, plugin='simpleitk')\n",
    "    #img = trans.resize(img, resize, mode='constant')\n",
    "    if label:\n",
    "        if label_num == 5:\n",
    "            img[img != 0] = 1       #Region 1 => 1+2+3+4 complete tumor\n",
    "        if label_num == 1:\n",
    "            img[img != 1] = 0       #only left necrosis\n",
    "        if label_num == 2:\n",
    "            img[img == 2] = 0       #turn edema to 0\n",
    "            img[img != 0] = 1       #only keep necrosis, ET, NET = Tumor core\n",
    "        if label_num == 4:\n",
    "            img[img != 4] = 0       #only left ET\n",
    "            img[img == 4] = 1\n",
    "        img = img.astype('float32')\n",
    "    else:\n",
    "        img = (img-img.mean()) / img.std()      #normalization => zero mean   !!!care for the std=0 problem\n",
    "        img = img.astype('float32')\n",
    "    for slice in range(155):     #choose the slice range\n",
    "        img_t = img[slice,:,:]\n",
    "        img_t =img_t.reshape((1,)+img_t.shape)\n",
    "        img_t =img_t.reshape((1,)+img_t.shape)   #become rank 4\n",
    "        #img_g = augmentation(img_t,num_of_aug)\n",
    "        for n in range(img_t.shape[0]):\n",
    "            imgs.append(img_t[n,:,:,:])\n",
    "    \n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bb1PxgjWQTyL"
   },
   "outputs": [],
   "source": [
    "def augmentation(scans,n):          #input img must be rank 4 \n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,   \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,  \n",
    "        rotation_range=25,   \n",
    "        width_shift_range=0.1,  \n",
    "        height_shift_range=0.1,   \n",
    "        horizontal_flip=True,   \n",
    "        vertical_flip=True,  \n",
    "        zoom_range=0.1)\n",
    "    i=0\n",
    "    scans_g=scans.copy()\n",
    "    for batch in datagen.flow(scans, batch_size=1, seed=1000): \n",
    "        scans_g=np.vstack([scans_g,batch])\n",
    "        i += 1\n",
    "        if i == n:\n",
    "            break\n",
    "    '''    remember arg + labels  \n",
    "    i=0\n",
    "    labels_g=labels.copy()\n",
    "    for batch in datagen.flow(labels, batch_size=1, seed=1000): \n",
    "        labels_g=np.vstack([labels_g,batch])\n",
    "        i += 1\n",
    "        if i > n:\n",
    "            break    \n",
    "    return ((scans_g,labels_g))'''\n",
    "    return scans_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhCb-4cMoGqL"
   },
   "outputs": [],
   "source": [
    "def create_graph(history, epoch, path):\n",
    "  #create list all data in history\n",
    "  print(history.history.keys())\n",
    "  # summarize history for accuracy\n",
    "  plt.plot(history.history['dice_coef'])\n",
    "  plt.plot(history.history['val_dice_coef'])\n",
    "  plt.title('model dice_coef')\n",
    "  plt.ylabel('dice_coef')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'validation'], loc='upper left')\n",
    "  plt.savefig('{}/dice_coef_{}.png'.format(path, epoch), dpi=300)\n",
    "  plt.show()\n",
    "  # summarize history for loss\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('model loss')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "  plt.savefig('{}/val_loss_{}.png'.format(path, epoch), dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVoEs0FAMWVK"
   },
   "outputs": [],
   "source": [
    "\n",
    "# our U-net for full tumor segmentation\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "def leaky_relu(x): return tf.nn.leaky_relu(x, alpha=0.2)\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "    \n",
    "def unet_model(input=1):\n",
    "    inputs = Input((input, img_size, img_size))\n",
    "    conv1 = Conv2D(64, (3, 3), activation=leaky_relu, padding='same') (inputs)\n",
    "    batch1 = BatchNormalization(axis=1)(conv1)\n",
    "    conv1 = Conv2D(64, (3, 3), activation=leaky_relu, padding='same') (batch1)\n",
    "    batch1 = BatchNormalization(axis=1)(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2)) (batch1)\n",
    "    \n",
    "    conv2 = Conv2D(128, (3, 3), activation=leaky_relu, padding='same') (pool1)\n",
    "    batch2 = BatchNormalization(axis=1)(conv2)\n",
    "    conv2 = Conv2D(128, (3, 3), activation=leaky_relu, padding='same') (batch2)\n",
    "    batch2 = BatchNormalization(axis=1)(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2)) (batch2)\n",
    "    \n",
    "    conv3 = Conv2D(256, (3, 3), activation=leaky_relu, padding='same') (pool2)\n",
    "    batch3 = BatchNormalization(axis=1)(conv3)\n",
    "    conv3 = Conv2D(256, (3, 3), activation=leaky_relu, padding='same') (batch3)\n",
    "    batch3 = BatchNormalization(axis=1)(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2)) (batch3)\n",
    "    \n",
    "    conv4 = Conv2D(512, (3, 3), activation=leaky_relu, padding='same') (pool3)\n",
    "    batch4 = BatchNormalization(axis=1)(conv4)\n",
    "    conv4 = Conv2D(512, (3, 3), activation=leaky_relu, padding='same') (batch4)\n",
    "    batch4 = BatchNormalization(axis=1)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2)) (batch4)\n",
    "    \n",
    "    conv5 = Conv2D(1024, (3, 3), activation=leaky_relu, padding='same') (pool4)\n",
    "    batch5 = BatchNormalization(axis=1)(conv5)\n",
    "    conv5 = Conv2D(1024, (3, 3), activation=leaky_relu, padding='same') (batch5)\n",
    "    batch5 = BatchNormalization(axis=1)(conv5)\n",
    "    \n",
    "    up6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same') (batch5)\n",
    "    up6 = concatenate([up6, conv4], axis=1)\n",
    "    conv6 = Conv2D(512, (3, 3), activation=leaky_relu, padding='same') (up6)\n",
    "    batch6 = BatchNormalization(axis=1)(conv6)\n",
    "    conv6 = Conv2D(512, (3, 3), activation=leaky_relu, padding='same') (batch6)\n",
    "    batch6 = BatchNormalization(axis=1)(conv6)\n",
    "    \n",
    "    up7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (batch6)\n",
    "    up7 = concatenate([up7, conv3], axis=1)\n",
    "    conv7 = Conv2D(256, (3, 3), activation=leaky_relu, padding='same') (up7)\n",
    "    batch7 = BatchNormalization(axis=1)(conv7)\n",
    "    conv7 = Conv2D(256, (3, 3), activation=leaky_relu, padding='same') (batch7)\n",
    "    batch7 = BatchNormalization(axis=1)(conv7)\n",
    "    \n",
    "    up8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (batch7)\n",
    "    up8 = concatenate([up8, conv2], axis=1)\n",
    "    conv8 = Conv2D(128, (3, 3), activation=leaky_relu, padding='same') (up8)\n",
    "    batch8 = BatchNormalization(axis=1)(conv8)\n",
    "    conv8 = Conv2D(128, (3, 3), activation=leaky_relu, padding='same') (batch8)\n",
    "    batch8 = BatchNormalization(axis=1)(conv8)\n",
    "    \n",
    "    up9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (batch8)\n",
    "    up9 = concatenate([up9, conv1], axis=1)\n",
    "    conv9 = Conv2D(64, (3, 3), activation=leaky_relu, padding='same') (up9)\n",
    "    batch9 = BatchNormalization(axis=1)(conv9)\n",
    "    conv9 = Conv2D(64, (3, 3), activation=leaky_relu, padding='same') (batch9)\n",
    "    batch9 = BatchNormalization(axis=1)(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(batch9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=LR), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmGcxYDMRZxW"
   },
   "outputs": [],
   "source": [
    "\n",
    "augmented = True\n",
    "#flair\n",
    "if os.path.isfile('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"flair\")) == False:\n",
    "  create_data(dataset_path, '**/*_flair.nii.gz', scanType=\"flair\", label=False, augmented=augmented, resize=(155,img_size,img_size))\n",
    "\n",
    "#t2\n",
    "if os.path.isfile('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"t2\")) == False:\n",
    "  create_data(dataset_path, '**/*_t2.nii.gz', scanType=\"t2\", label=False, augmented=augmented, resize=(155,img_size,img_size))\n",
    "\n",
    "\n",
    "#segmentation Full Tumor\n",
    "if os.path.isfile('{}/y_{}_{}.npy'.format(destination_numpy, img_size, 5)) == False:\n",
    "  create_data(dataset_path, '**/*_seg.nii.gz', label=True, label_num = 5, augmented=augmented, resize=(155,img_size,img_size))\n",
    "\n",
    "#Load other module BRATS (t1ce and ET, Core GT)\n",
    "#t1\n",
    "if os.path.isfile('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"t1\")) == False:\n",
    "  create_data(dataset_path, '**/*_t1.nii.gz', scanType=\"t1\", label=False, augmented=augmented, resize=(155,img_size,img_size))\n",
    "\n",
    "#t1ce\n",
    "if os.path.isfile('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"t1ce\")) == False:\n",
    "  create_data(dataset_path, '**/*_t1ce.nii.gz', scanType=\"t1ce\", label=False, augmented=augmented, resize=(155,img_size,img_size))\n",
    "\n",
    "#segmentation Core Tumor\n",
    "if os.path.isfile('{}/y_{}_{}.npy'.format(destination_numpy,img_size, 2)) == False:\n",
    "  create_data(dataset_path, '**/*_seg.nii.gz', label=True, label_num = 2, augmented=augmented, resize=(155,img_size,img_size))\n",
    "\n",
    "#segmentation ET Tumor\n",
    "if os.path.isfile('{}/y_{}_{}.npy'.format(destination_numpy, img_size, 4)) == False:\n",
    "  create_data(dataset_path, '**/*_seg.nii.gz', label=True, label_num = 4, augmented=augmented, resize=(155,img_size,img_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovlxK0oaqIqA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flair = np.load('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"flair\"))\n",
    "T1 = np.load('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"t1\"))\n",
    "T2 = np.load('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"t2\"))\n",
    "T1c = np.load('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"t1ce\"))\n",
    "\n",
    "x_train = np.hstack((Flair, T1, T2, T1c))\n",
    "del Flair\n",
    "del T1\n",
    "del T2\n",
    "del T1c\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iechoHXoMWVO",
    "outputId": "63a27b8a-2d52-454e-dfa3-c93411bfe533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/anacaraka_info/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "loss: 0.13%\n",
      "dice_coef: 86.71%\n"
     ]
    }
   ],
   "source": [
    "y_train = np.load('{}/y_{}_{}.npy'.format(destination_numpy, img_size, 5))\n",
    "checkpoint_path = '{}/{}/weight_full_best.h5'.format(destination_model, experiment)\n",
    "isBuildNewModel = True if os.path.isfile(checkpoint_path) == False else False\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model = unet_model(4)\n",
    "if isBuildNewModel == True:\n",
    "  history = model.fit(x_train, y_train, batch_size=batch_size, callbacks=[model_checkpoint, early_stop], validation_split=0.2, epochs= num_epoch, verbose=verbose, shuffle=True)\n",
    "  create_graph(history, start_epoch, '{}/full'.format(result_destination))\n",
    "  pandas.DataFrame(history.history).to_csv(\"{}/history_full_{}.csv\".format(result_destination, start_epoch))\n",
    "  #model.save(model_checkpoint)\n",
    "else:\n",
    "  model = load_model(checkpoint_path, custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef, 'leaky_relu': leaky_relu})\n",
    "  if continue_training == False :\n",
    "    scores = model.evaluate(x_train, y_train, verbose = 0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "  else:\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, callbacks=[model_checkpoint, early_stop], validation_split=0.2, epochs= num_epoch, initial_epoch=start_epoch, verbose=verbose, shuffle=True)\n",
    "    create_graph(history, start_epoch, '{}/full'.format(result_destination))\n",
    "    pandas.DataFrame(history.history).to_csv(\"{}/history_full_{}.csv\".format(result_destination, start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s39_gVNgBI3S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del y_train\n",
    "#del x_train\n",
    "gc.collect()\n",
    "\n",
    "#T1c = np.load('{}/x_{}_{}.npy'.format(destination_numpy, img_size, \"t1ce\"))\n",
    "#x_train = np.hstack(( T2, T1c))\n",
    "#del T2\n",
    "#del T1c\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vuu1rALnC5-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.24%\n",
      "dice_coef: 76.10%\n"
     ]
    }
   ],
   "source": [
    "y_train = np.load('{}/y_{}_{}.npy'.format(destination_numpy, img_size, 4))\n",
    "checkpoint_path = '{}/{}/weight_et_best.h5'.format(destination_model, experiment)\n",
    "isBuildNewModel = True if os.path.isfile(checkpoint_path) == False else False\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model_ET = unet_model(4)\n",
    "if isBuildNewModel == True:\n",
    "  history = model_ET.fit(x_train, y_train, batch_size=batch_size, callbacks=[model_checkpoint, early_stop], validation_split=0.2, epochs= num_epoch, verbose=verbose, shuffle=True)\n",
    "  create_graph(history, start_epoch, '{}/et'.format(result_destination))\n",
    "  pandas.DataFrame(history.history).to_csv(\"{}/history_et_{}.csv\".format(result_destination, start_epoch))\n",
    "else:\n",
    "  model_ET = load_model(checkpoint_path, custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef, 'leaky_relu': leaky_relu})\n",
    "  if continue_training == False :\n",
    "    scores = model_ET.evaluate(x_train, y_train, verbose = 0)\n",
    "    print(\"%s: %.2f%%\" % (model_ET.metrics_names[0], scores[0]))\n",
    "    print(\"%s: %.2f%%\" % (model_ET.metrics_names[1], scores[1]*100))\n",
    "  else:\n",
    "    history = model_ET.fit(x_train, y_train, batch_size=batch_size, callbacks=[model_checkpoint, early_stop], validation_split=0.2, epochs= num_epoch, initial_epoch=start_epoch, verbose=verbose, shuffle=True)\n",
    "    create_graph(history, start_epoch, '{}/et'.format(result_destination))\n",
    "    pandas.DataFrame(history.history).to_csv(\"{}/history_et_{}.csv\".format(result_destination, start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KubxGoAHOM7r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wq1RcUF5TyO"
   },
   "outputs": [],
   "source": [
    "y_train = np.load('{}/y_{}_{}.npy'.format(destination_numpy, img_size, 2))\n",
    "checkpoint_path = '{}/{}/weight_core_best.h5'.format(destination_model, experiment)\n",
    "isBuildNewModel = True if os.path.isfile(checkpoint_path) == False else False\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model_core = unet_model(4)\n",
    "if isBuildNewModel == True:\n",
    "  history = model_core.fit(x_train, y_train, batch_size=batch_size, callbacks=[model_checkpoint, early_stop], validation_split=0.2, epochs= num_epoch, verbose=verbose, shuffle=True)\n",
    "  create_graph(history, start_epoch, '{}/core'.format(result_destination))\n",
    "  pandas.DataFrame(history.history).to_csv(\"{}/history_core_{}.csv\".format(result_destination, start_epoch))\n",
    "else:\n",
    "  model_core = load_model(checkpoint_path, custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef, 'leaky_relu': leaky_relu})\n",
    "  if continue_training == False :\n",
    "    scores = model_core.evaluate(x_train, y_train, verbose = 0)\n",
    "    print(\"%s: %.2f%%\" % (model_core.metrics_names[0], scores[0]))\n",
    "    print(\"%s: %.2f%%\" % (model_core.metrics_names[1], scores[1]*100))\n",
    "  else:\n",
    "    history = model_core.fit(x_train, y_train, batch_size=batch_size, callbacks=[model_checkpoint, early_stop], validation_split=0.2, epochs= num_epoch, initial_epoch=start_epoch, verbose=verbose, shuffle=True)\n",
    "    create_graph(history, start_epoch, '{}/core'.format(result_destination))\n",
    "    pandas.DataFrame(history.history).to_csv(\"{}/history_core_{}.csv\".format(result_destination, start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgtVa5jSXAH5"
   },
   "outputs": [],
   "source": [
    "del x_train\n",
    "del y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "badR6dBtXaoG"
   },
   "outputs": [],
   "source": [
    "#read one subject to show slices\n",
    "#path_hgg = '{}/HGG/'.format(dataset_path)\n",
    "path_hgg = \"{}/{}/{}/\".format(dataset_path, \"HGG\",\"Brats17_TCIA_430_1\")\n",
    "count = 1 #number of files on HGG\n",
    "pul_seq = 'flair'\n",
    "Flair_Predict = create_data_onesubject_val(path_hgg, '**/*{}.nii.gz'.format(pul_seq), count, label=False)\n",
    "pul_seq = 't1ce'\n",
    "T1c_Predict = create_data_onesubject_val(path_hgg, '**/*{}.nii.gz'.format(pul_seq), count, label=False)\n",
    "pul_seq = 't1'\n",
    "T1_Predict = create_data_onesubject_val(path_hgg, '**/*{}.nii.gz'.format(pul_seq), count, label=False)\n",
    "pul_seq = 't2'\n",
    "T2_Predict = create_data_onesubject_val(path_hgg, '**/*{}.nii.gz'.format(pul_seq), count, label=False)\n",
    "\n",
    "# Full Segmentation Tumor\n",
    "Label_full_Predict = create_data_onesubject_val(path_hgg, '**/*seg.nii.gz', count, label_num = 5, label=True)\n",
    "#Core Segmentation Tumor\n",
    "Label_core_Predict = create_data_onesubject_val(path_hgg, '**/*seg.nii.gz', count, label_num = 2, label=True)\n",
    "#ET Segmentation Tumor\n",
    "Label_ET_Predict = create_data_onesubject_val(path_hgg, '**/*seg.nii.gz', count, label_num = 4, label=True)\n",
    "#Join All Segmentation\n",
    "Label_all_Predict = create_data_onesubject_val(path_hgg, '**/*seg.nii.gz', count, label_num = 3, label=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(241)\n",
    "plt.title('T1')\n",
    "plt.axis('off')\n",
    "plt.imshow(T1_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(242)\n",
    "plt.title('T2')\n",
    "plt.axis('off')\n",
    "plt.imshow(T2_Predict[90, 0, :, :],cmap='gray')\n",
    "    \n",
    "plt.subplot(243)\n",
    "plt.title('Flair')\n",
    "plt.axis('off')\n",
    "plt.imshow(Flair_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(244)\n",
    "plt.title('T1c')\n",
    "plt.axis('off')\n",
    "plt.imshow(T1c_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(245)\n",
    "plt.title('Ground Truth(Full)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_full_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(246)\n",
    "plt.title('Ground Truth(Core)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_core_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(247)\n",
    "plt.title('Ground Truth(ET)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_ET_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(248)\n",
    "plt.title('Ground Truth(All)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_all_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('{}/tumor.png'.format(result_destination), dpi=300)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PnquyjgaCUQ6"
   },
   "outputs": [],
   "source": [
    "#x1 = np.zeros((1,4,240,240),np.float32)\n",
    "#x1[:,:1,:,:] = Flair_Predict[89:90,:,:,:]  #choosing 90th slice as example\n",
    "#x1[:,1:,:,:] = T2_Predict[89:90,:,:,:] \n",
    "x1 = np.hstack((Flair_Predict[89:90,:,:,:], T1_Predict[89:90,:,:,:], T2_Predict[89:90,:,:,:], T1c_Predict[89:90,:,:,:]))\n",
    "pred_full = model.predict(x1)\n",
    "\n",
    "#x2 = np.zeros((1,4,240,240),np.float32)\n",
    "#x2[:,:1,:,:] = T2_Predict[89:90,:,:,:]  #choosing 90th slice as example\n",
    "#x2[:,1:,:,:] = T1c_Predict[89:90,:,:,:] \n",
    "#pred_full = model.predict(x1)\n",
    "\n",
    "pred_core = model_core.predict(x1)\n",
    "pred_ET = model_ET.predict(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlueyGQyMWVu"
   },
   "outputs": [],
   "source": [
    "def paint_color_algo(pred_full, pred_core , pred_ET ):   #input image is [n,1, y, x]\n",
    "    # first put the pred_full on T1c\n",
    "    pred_full[pred_full > 0.2] = 2      #240x240\n",
    "    pred_full[pred_full != 2] = 0\n",
    "    pred_core[pred_core > 0.2] = 1      #64x64\n",
    "    pred_core[pred_core != 1] = 0\n",
    "    pred_ET[pred_ET > 0.2] = 4          #64x64\n",
    "    pred_ET[pred_ET != 4] = 0\n",
    "\n",
    "    total = np.zeros((1,240,240),np.float32)  \n",
    "    total[:,:,:] = pred_full[:,:,:]\n",
    "    for i in range(pred_core.shape[0]):\n",
    "        for j in range(240):\n",
    "            for k in range(240):\n",
    "                if pred_core[i,j,k] != 0 and pred_full[i,j,k] !=0:\n",
    "                    total[i,j,k] = pred_core[i,j,k]\n",
    "                if pred_ET[i,j,k] != 0 and pred_full[i,j,k] !=0:\n",
    "                    total[i,j,k] = pred_ET[i,j,k]\n",
    "                \n",
    "    \n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cw0lGREzMWVw"
   },
   "outputs": [],
   "source": [
    "tmp = paint_color_algo(pred_full[0,:,:,:], pred_core[0,:,:,:], pred_ET[0,:,:,:])\n",
    "\n",
    "core = np.zeros((1,240,240),np.float32)\n",
    "ET = np.zeros((1,240,240),np.float32)\n",
    "core[:,:,:] = tmp[:,:,:]\n",
    "ET[:,:,:] = tmp[:,:,:]\n",
    "core[core == 4] = 1\n",
    "core[core != 1] = 0\n",
    "ET[ET != 4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xP88eBbMWV0"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(341)\n",
    "plt.title('T1')\n",
    "plt.axis('off')\n",
    "plt.imshow(T1_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(342)\n",
    "plt.title('T2')\n",
    "plt.axis('off')\n",
    "plt.imshow(T2_Predict[90, 0, :, :],cmap='gray')\n",
    "    \n",
    "plt.subplot(343)\n",
    "plt.title('Flair')\n",
    "plt.axis('off')\n",
    "plt.imshow(Flair_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(344)\n",
    "plt.title('T1c')\n",
    "plt.axis('off')\n",
    "plt.imshow(T1c_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(345)\n",
    "plt.title('Ground Truth(Full)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_full_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(346)\n",
    "plt.title('Ground Truth(Core)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_core_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(347)\n",
    "plt.title('Ground Truth(ET)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_ET_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(348)\n",
    "plt.title('Ground Truth(All)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Label_all_Predict[90, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(349)\n",
    "plt.title('Prediction (Full)')\n",
    "plt.axis('off')\n",
    "plt.imshow(pred_full[0, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(3,4,10)\n",
    "plt.title('Prediction (Core)')\n",
    "plt.axis('off')\n",
    "plt.imshow(core[0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(3,4,11)\n",
    "plt.title('Prediction (ET)')\n",
    "plt.axis('off')\n",
    "plt.imshow(ET[0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(3,4,12)\n",
    "plt.title('Prediction (All)')\n",
    "plt.axis('off')\n",
    "plt.imshow(tmp[0, :, :],cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('{}/all_segmentation_{}.png'.format(result_destination, start_epoch))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIpbpfgmMWV3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BraTS2017_LReLU_Bias.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
